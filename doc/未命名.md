There is a lot of changes between the design doc and the real code. 


1. Initially we create a  tree data structure to represent the priority queue which is very troublesome but very intuitive. And we use this  data structure to maintian a heap which will always pop out the threads with max priority and minimize the overhead of `next _thread_to_run` in the long run.   However in the real code we abandon the new tree data structure and we use existing linked list to organize the threads with different priority, specificlly the `all_list` and `ready_list`.
2. we abandon the priority queue which is unecessary for pintos with few thread running simultaneously. we just change the strategy of the function `next _thread_to_run` . Basically what we do is traversing the `ready_list` and pop out the `list_elem` with biggest priority.
3. Adding the `thread_yield` calling at the end of function `create_thread` and `thread_set_priority` which is neglected when we work on design doc.weadd these two function in case of two situations. (1).A low priority thread create a high priority thread and low thread need to yield the cpu immediately.(2). A high priority thread set it's own priority lower than the second biggest priority in `ready_list` and in this situation current thread is required to yield the cpu immediately.
4. Adding some member variables in `struct thread` to track priority donation. My orginal design is to use local variable to record every priority donation. However whenwereally implement the functions,it turns out that we have to change a priority and restore it in two different functions.I could use global variable to record the donation but it could make code very complicated  damaging the readability of code. we choose to record the priority dontion within thread because the priority donation only affect the thread which receive the donation.
5. Changing the stratrety of finding the next thread in the waiting list of condition variables, lock and semaphore to satisfy the requirement of thread with highest priority.However we didn't notice is when we work on our design doc and we only change the strategy of `next_thread_to_run`.
6. We use `thread_foreach` to check all the thread in the `all_list`  to find the owner a of lock and if we find the owner of a lock we do priority donation  if we can't do nothing which is different comparing we thought in design doc.We thought we could create a new static list `lock_list` to record the thread which own a lock and every time we check the owner of lock we just need to go through the `lock_list`.However we couldn't find out the reason why the `lock_list` couldn't be initialize correctly and every time i loop the `lock_list` it always can't jump out of the loop. So we have to use the existing threads.
7. The member variable `own_lock ` and `orginal_priority` was designed to restore the the original priority.Now we reuse them but with different purposes.`lock_own` is to denote the number of priority donation record stored in `priority_donation[MAX_DONATION_NUM]` and `orginal_priority` is to store the priority when `thread_set_priority` is called but the current thread owns locks.
8. We add the pointer `waiting_lock` to denote the lock that a thread is waiting for which  is not included in doc.We add  the waiting lock to handle the situation like the "donation chain".

Reflection and improvment

The scheduler works fine though its time complexity is O(n) but for the limited thread in pintos ,it will not cost much to check all the thread in `ready_list`.

We don't have to go through all the threads recorded in `all_list`  to find the owner of lock, actually we could create a new list `lock_list` to record the owner of the list which may save a lot time, especially if there is a lot of thread in system but few of them owns a lock.

